{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Deploy the model as Batch Endpoint\n",
        "\n",
        "##### Workflow\n",
        "1. Initialize \n",
        "- Workspace\n",
        "- Environment\n",
        "- Cluster\n",
        "- Experiment\n",
        "2. Get Reference to Input data\n",
        "3. Create a Scoring script\n",
        "4. Create and Submit Pipeline Step\n",
        "5. Download the predictions to local folder (optional)\n",
        "6. Publish the pipeline\n",
        "7. Invoke pipeline endpoint\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Initializing Workspace\n",
        "\n",
        "> **Note**: If you haven't already established an authenticated session with your Azure subscription, you'll be prompted to authenticate by clicking a link, entering an authentication code, and signing into Azure."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "# Initializing Workspace\n",
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1699975318469
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "compute = \"ML-Pipeline-Cluster\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    inference_cluster = ComputeTarget(workspace=ws, name=compute)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, create it\n",
        "    try:\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
        "        inference_cluster = ComputeTarget.create(ws, compute, compute_config)\n",
        "        inference_cluster.wait_for_completion(show_output=True)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699975323155
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create or Get Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an environment\n",
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "# ---- create enviroment using .yaml file\n",
        "env_name= 'ENV-SDKv1'\n",
        "# python_packages = Environment.from_conda_specification(env_name, '../dependencies/conda.yaml')\n",
        "# # register the environment\n",
        "# python_packages.register(workspace=ws)\n",
        "\n",
        "# calling Environment\n",
        "reg_env = Environment.get(ws, env_name)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699975327018
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Get Referrence to Input Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "# ---- Getting Data\n",
        "dataset_name = 'pima_test_typeTabular_SDKv1'\n",
        "# loading data from Dataset\n",
        "df_tb   = Dataset.get_by_name(workspace=ws, name= dataset_name)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699975340092
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Create a Scoring Script\n",
        "The Scoring script must contain a **init()** & **run(mini_batch)** function\n",
        "- **run(mini_batch)**: The function will run for each mini_batch instance.\n",
        "- **mini_batch**: ParallelRunStep will invoke run method and pass either a list or pandas DataFrame as an argument to the method. Each entry in mini_batch will be a **file path** if input is a **FileDataset** or a **pandas DataFrame** if input is a **TabularDataset**\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##### Step 3: Create a pipeline for batch inferencing\n",
        "You're going to use a pipeline to run the batch prediction script, generate predictions from the input data, and save the results as a text file in the output folder. To do this, you can use a **ParallelRunStep**, which enables the batch data to be processed in parallel and the results collated in a single output file named *parallel_run_step.txt*.\n",
        "\n",
        "**Important**: For more details on Batch Inferencing & scoring script, [click here](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-debug-parallel-run-step?view=azureml-api-1#testing-scripts-locally)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "\n",
        "output_dir = OutputFileDatasetConfig(name='Pima_BatchEndpoint_Output')\n",
        "\n",
        "parallel_run_config = ParallelRunConfig(\n",
        "    source_directory='../src',\n",
        "    entry_script=\"pima_scoreBatchEndpoint_SDKv1.py\",\n",
        "    mini_batch_size='10MB',\n",
        "    error_threshold=-1,\n",
        "    output_action=\"append_row\",\n",
        "    environment= reg_env,\n",
        "    compute_target=compute,\n",
        "    node_count=2\n",
        "    )\n",
        "\n",
        "parallelrun_step = ParallelRunStep(\n",
        "    name='pima-batch-endpoint-SDKv1',\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    inputs=[df_tb.as_named_input('pima_batch_data')],\n",
        "    output=output_dir,\n",
        "    # arguments=[],\n",
        "    allow_reuse=False\n",
        ")\n",
        "\n",
        "print('Steps defined')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Steps defined\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/_parallel_run_step_base.py:580: UserWarning: \nParallelRunStep requires azureml-dataset-runtime[fuse,pandas] for tabular dataset.\nPlease add relevant package in CondaDependencies.\n  warnings.warn(\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699975923200
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "put the parallelrun_step into a pipeline, and run it.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
        "pipeline_run = Experiment(ws, 'Pima_Batch_Experiments_Training_SDK_v1').submit(pipeline)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step pima-batch-endpoint-SDKv1 [4fe241a0][4251a177-13e8-4bf4-97d8-0bc91e4ff5fb], (This step will run and generate new outputs)\nSubmitted PipelineRun 96ae44f8-6ac2-4cc9-b821-dd655a79e73f\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/96ae44f8-6ac2-4cc9-b821-dd655a79e73f?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure&tid=5ac231ff-07da-46e9-9b1d-c924625f23bd\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699975926563
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 5: Download  predictions to local Folder (Optional)\n",
        "When the pipeline has finished running, the resulting predictions will have been saved in the outputs of the experiment associated with the first (and only) step in the pipeline. You can retrieve it as follows:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Get the run for the first step and download its output\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('Pima_BatchEndpoint_Output')\n",
        "prediction_output.download(local_path='../batchprediction_output')\n",
        "\n",
        "# Traverse the folder hierarchy and find the results file\n",
        "for root, dirs, files in os.walk('../batchprediction_output'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# cleanup output format\n",
        "df = pd.read_csv(result_file, delimiter=\" \", header=None)\n",
        "\n",
        "# add column namesto dataframe\n",
        "df_main = df_tb.to_pandas_dataframe()\n",
        "#df_main.drop(['Time','Amount','Class'], axis=1, inplace=True)\n",
        "df_col   = df_main.columns.tolist()\n",
        "pred_col = ['Prediction']\n",
        "\n",
        "# Display the first 20 results\n",
        "df.columns = df_col + pred_col\n",
        "df.head(20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 6: Publish the Pipeline\n",
        "\n",
        "Now that you have a working pipeline for batch inferencing, you can publish it and use a REST endpoint to run it from an application."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(name='pima_pipelineEndpoint_BatchPrediction_SDKv1', description='Batch scoring of diabetes data', version='1.0')\n",
        "# Get pipeline endpoint\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "print(rest_endpoint)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 7: Invoke the pipeline endpoint"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import requests\n",
        "\n",
        "# Authenticate\n",
        "interactive_auth = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_auth.get_authentication_header()\n",
        "print('Authentication header ready.')\n",
        "\n",
        "\n",
        "# Invoke\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=auth_header, \n",
        "                         json={\"ExperimentName\": \"Pima_Batch_Experiments_Training_SDK_v1\"})\n",
        "run_id = response.json()[\"Id\"]\n",
        "run_id"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}