{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Workflow\n",
        "1. Initialize \n",
        "    - Workspace\n",
        "    - Default Datastore (to store pipeline step output)\n",
        "    - compute Cluster (to run the pipeline)\n",
        "    - Environment\n",
        "    - Experiment\n",
        "2. Get Input Data\n",
        "3. create Pipeline Output folders to store pipeline step output\n",
        "4. create pipeline steps\n",
        "5. Submit Pipeline\n",
        "6. Create an Endpoint and Publish Pipeline \n",
        "7. Invoke Pipeline \n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Initialize Workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "#Initialize Workspace\n",
        "ws = Workspace.from_config()\n",
        "datastore =  ws.get_default_datastore() # to store pipeline data output, mandatory to define a default datastore "
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1699864098873
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Get Or Create Compute Cluster"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Initialize Compute Target\n",
        "# Choose a name for your CPU cluster\n",
        "compute = 'ML-Pipeline-Cluster'\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=compute)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                           max_nodes=4,\n",
        "                                                           idle_seconds_before_scaledown=300)\n",
        "    cpu_cluster = ComputeTarget.create(ws, compute, compute_config)\n",
        "\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1699864102897
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Create or Get Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "# creating an environment\n",
        "env_name = 'ENV-SDKv1'\n",
        "\n",
        "# ---- Create environment using defining packages\n",
        "# custom_env = Environment('aml-scikit-learn')\n",
        "# conda_dep = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy','pip', 'pyodbc','sqlalchemy'],\n",
        "#                                      pip_packages=['azureml-defaults'])\n",
        "\n",
        "# custom_env.python.conda_dependencies = conda_dep\n",
        "# register the environment\n",
        "# custom_env.register(workspace=ws)\n",
        "\n",
        "# ---- create enviroment using .yaml file\n",
        "##python_packages = Environment.from_conda_specification(env_name, '../dependencies/conda.yaml')\n",
        "# register the environment\n",
        "##python_packages.register(workspace=ws)\n",
        "\n",
        "# # calling registered environent\n",
        "reg_env = Environment.get(ws, env_name)\n",
        "\n",
        "# create a run config object for the pipeline\n",
        "pipeline_runconfig = RunConfiguration()\n",
        "\n",
        "# # use the compute target\n",
        "pipeline_runconfig.target = compute\n",
        "\n",
        "# # assigning the run configuration to the envrionment\n",
        "pipeline_runconfig.environment = reg_env\n",
        "print('RunConfiguration created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunConfiguration created\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1699864110359
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "# create an experiment\n",
        "experiment_name = 'Pima_Training_pipeline_SDKv1'\n",
        "experiment = Experiment(workspace = ws, name = experiment_name)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1699864126676
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 2 : Get Input Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "# Get Input Data\n",
        "dataset_name = 'pima-sdk-v1' \n",
        "df_tb   = Dataset.get_by_name(workspace=ws, name= dataset_name) # loading data from Dataset"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1699864157848
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Create pipeline Output folder"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\n",
        "\n",
        "# creating a output folder\n",
        "pre_process_output_folder = PipelineData(name='pre_process' , datastore=datastore)\n",
        "post_process_output_folder= PipelineData(name='post_process',  datastore=datastore)\n",
        "final_output_folder  = PipelineData(name='final_output',  datastore=datastore)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1699864163577
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Pipeline Steps"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "# creating pipeline steps\n",
        "pre_process_step = PythonScriptStep(name = 'step 1: Data Preparation', \n",
        "                                    script_name='pima_dataProcessing_SDKv1.py', \n",
        "                                    arguments= [\n",
        "                                                '--input_data'  , df_tb.as_named_input('raw_data'),\n",
        "                                                '--train_test_ratio', 0.3, \n",
        "                                                '--output', pre_process_output_folder],   \n",
        "                                    outputs  = [pre_process_output_folder],\n",
        "                                    compute_target=compute, \n",
        "                                    runconfig=pipeline_runconfig, \n",
        "                                    allow_reuse=False, \n",
        "                                    source_directory='../src')\n",
        "\n",
        "model_training_step = PythonScriptStep(name = 'step 2: Model Training', \n",
        "                                    script_name='pima_modelTraining_SDKv1.py',\n",
        "                                    arguments= ['--input_data',pre_process_output_folder, \n",
        "                                                \n",
        "                                                '--output', post_process_output_folder], \n",
        "                                    inputs   = [pre_process_output_folder], \n",
        "                                    outputs  = [post_process_output_folder], \n",
        "                                    compute_target=compute, \n",
        "                                    runconfig=pipeline_runconfig, \n",
        "                                    allow_reuse=False, \n",
        "                                    source_directory='../src') \n",
        "\n",
        "model_register_step = PythonScriptStep(name = 'step 3: Model Evaluation', \n",
        "                                script_name='pima_modelRegister_SDKv1.py',\n",
        "                                arguments= ['--actual_prediction_data',post_process_output_folder,\n",
        "                                            '--output', final_output_folder\n",
        "                                           ], \n",
        "                                inputs  = [post_process_output_folder], \n",
        "                                outputs = [final_output_folder], \n",
        "                                compute_target=compute, \n",
        "                                runconfig=pipeline_runconfig, \n",
        "                                allow_reuse=False,\n",
        "                                source_directory='../src')\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[pre_process_step,model_training_step, model_register_step]) \n",
        "pipeline.validate()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step step 1: Data Preparation is ready to be created [696c67ee]\nStep step 2: Model Training is ready to be created [a51d3e70]\nStep step 3: Model Evaluation is ready to be created [ba52bb4a]\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1697525959930
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 5: Submit Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pipeline as an experiment\n",
        "pipeline_run = Experiment(ws, 'Experiments_Training').submit(pipeline, continue_on_step_failure=True,)\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step step 1: Data Preparation [1baa8bb4][ea29a82e-e5bb-489b-9065-98883c8fbedb], (This step will run and generate new outputs)\nCreated step step 2: Model Training [ebc99b00][900816ca-c26f-40f2-b2b8-6370d49f0e20], (This step will run and generate new outputs)\nCreated step step 3: Model Evaluation [f0cae487][8746545c-9296-4247-ad99-7b5e8a7f591e], (This step will run and generate new outputs)\nSubmitted PipelineRun 61365c11-2c8d-4128-b085-9c46b183aab4\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/61365c11-2c8d-4128-b085-9c46b183aab4?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure&tid=5ac231ff-07da-46e9-9b1d-c924625f23bd\nPipelineRunId: 61365c11-2c8d-4128-b085-9c46b183aab4\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/61365c11-2c8d-4128-b085-9c46b183aab4?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure&tid=5ac231ff-07da-46e9-9b1d-c924625f23bd\nPipelineRun Status: NotStarted\nPipelineRun Status: Running\n\n\nStepRunId: 42441a17-3e01-4086-9d49-262fa0326bd0\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/42441a17-3e01-4086-9d49-262fa0326bd0?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure&tid=5ac231ff-07da-46e9-9b1d-c924625f23bd\nStepRun( step 1: Data Preparation ) Status: NotStarted\nStepRun( step 1: Data Preparation ) Status: Running\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1696429744322
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 6: Creating an endpoint and publishing the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineEndpoint\n",
        "\n",
        "# The pipeline argument can be either a Pipeline or a PublishedPipeline\n",
        "pipeline_endpoint = PipelineEndpoint.publish(workspace=ws,\n",
        "                                             name=\"training_PipelineEndpoint\",\n",
        "                                             pipeline=pipeline,\n",
        "                                             description=\"pipeline to train model\")\n",
        "\n",
        "                                               "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step step 1: Data Preparation [696c67ee][0c3b3f47-5c75-4d2a-aa86-d67b2facdb07], (This step will run and generate new outputs)\nCreated step step 2: Model Training [a51d3e70][d33a7a16-c941-48bc-8809-97b62951ed3a], (This step will run and generate new outputs)\nCreated step step 3: Model Evaluation [ba52bb4a][c0c28fdb-6510-4004-816e-a3aca125b2b7], (This step will run and generate new outputs)\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1696481067225
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## publish it to same endpoint when the pipeline is enhanced or modified\n",
        "\n",
        "# published_pipeline = pipeline_run.publish_pipeline(name = 'EnhancedTrainingPipelline',\n",
        "#                                                    description='pipeline to train model',\n",
        "#                                                    version='2')\n",
        "\n",
        "# pipeline_endpoint = PipelineEndpoint.get(workspace=ws, name=\"training_PipelineEndpoint\")\n",
        "# pipeline_endpoint.add_default(published_pipeline)\n",
        "# print(pipeline_endpoint.endpoint)\n",
        "# pipeline_endpoint"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696481133270
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 7: Invoking Endpoint\n",
        "\n",
        "To run the pipeline from the REST endpoint, you first need an OAuth2 Bearer-type authentication header. This example uses interactive authentication for illustration purposes, but for most production scenarios requiring automated or headless authentication, use service principle authentication as described in this notebook.\n",
        "\n",
        "Service principle authentication involves creating an App Registration in Azure Active Directory, generating a client secret, and then granting your service principal role access to your machine learning workspace. You then use the ServicePrincipalAuthentication class to manage your auth flow.\n",
        "\n",
        "Both InteractiveLoginAuthentication and ServicePrincipalAuthentication inherit from AbstractAuthentication, and in both cases you use the get_authentication_header() function in the same way to fetch the header"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import requests\n",
        "\n",
        "# Authentication\n",
        "interactive_authentication = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_authentication.get_authentication_header()\n",
        "print('authentication header ready')\n",
        "\n",
        "response = requests.post(pipeline_endpoint.endpoint,\n",
        "                         headers=auth_header,\n",
        "                         json={\"ExperimentName\": \"Experiments_Training\"})\n",
        "run_id = response.json()[\"Id\"]\n",
        "# print('pipeline invoked:',)\n",
        "print(auth_header)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696520844643
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments[\"Experiments_Training\"], run_id)\n",
        "published_pipeline_run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696481266660
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}