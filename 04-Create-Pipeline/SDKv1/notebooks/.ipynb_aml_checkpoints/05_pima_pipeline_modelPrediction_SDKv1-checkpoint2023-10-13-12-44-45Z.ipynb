{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Workflow\n",
        "1. Initialize \n",
        "    - Workspace\n",
        "    - Default Datastore (to store pipeline step output)\n",
        "    - compute Cluster (to run the pipeline)\n",
        "    - Environment\n",
        "    - Experiment\n",
        "2. Get Input Data\n",
        "3. create Pipeline Output folders to store pipeline step output\n",
        "4. create pipeline steps\n",
        "5. Submit Pipeline\n",
        "6. Create an Endpoint and Publish Pipeline \n",
        "7. Invoke Pipeline \n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Initialize Workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Workspace\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "datastore =  ws.get_default_datastore() # to store pipeline data output, mandatory to define a default datastore "
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1699877593229
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Get Or Create Compute Cluster"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Compute Target\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your CPU cluster\n",
        "compute = 'ML-Pipeline-Cluster'\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=compute)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                           max_nodes=4,\n",
        "                                                           idle_seconds_before_scaledown=300)\n",
        "    cpu_cluster = ComputeTarget.create(ws, compute, compute_config)\n",
        "\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1699877596089
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Create or Get Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an environment\n",
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "env_name = 'ENV-SDKv1'\n",
        "\n",
        "# ---- Create environment using defining packages\n",
        "# custom_env = Environment('aml-scikit-learn')\n",
        "# conda_dep = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy','pip', 'pyodbc','sqlalchemy'],\n",
        "#                                      pip_packages=['azureml-defaults'])\n",
        "\n",
        "# custom_env.python.conda_dependencies = conda_dep\n",
        "# register the environment\n",
        "# custom_env.register(workspace=ws)\n",
        "\n",
        "# ---- create enviroment using .yaml file\n",
        "python_packages = Environment.from_conda_specification(env_name, '../dependencies/conda.yaml')\n",
        "# register the environment\n",
        "python_packages.register(workspace=ws)\n",
        "\n",
        "# # calling registered environent\n",
        "reg_env = Environment.get(ws, env_name)\n",
        "\n",
        "# create a run config object for the pipeline\n",
        "pipeline_runconfig = RunConfiguration()\n",
        "\n",
        "# # use the compute target\n",
        "pipeline_runconfig.target = compute\n",
        "\n",
        "# # assigning the run configuration to the envrionment\n",
        "pipeline_runconfig.environment = reg_env\n",
        "print('RunConfiguration created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunConfiguration created\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1699877602842
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "# create an experiment\n",
        "experiment_name = 'Pima_Inference_pipeline_SDKv1'\n",
        "experiment = Experiment(workspace = ws, name = experiment_name)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1699877612366
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 2 : Get Input Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "# Get Input Data\n",
        "dataset_name = 'pima_test_typeTabular_SDKv1'\n",
        "df_tb   = Dataset.get_by_name(workspace=ws, name= dataset_name) # loading data from Dataset"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1699877618602
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Create pipeline Output folder"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\n",
        "\n",
        "# creating a output folder\n",
        "pre_process_output_folder = PipelineData(name='pre_process' , datastore=datastore)\n",
        "final_output_folder  = PipelineData(name='final_output',  datastore=datastore)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1699879186550
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Pipeline Steps"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "# creating pipeline steps\n",
        "pre_process_step = PythonScriptStep(name = 'step 1: Data Preparation', \n",
        "                                    script_name='pima_inference_dataProcessing_SDKv1.py', \n",
        "                                    arguments= [\n",
        "                                                '--input_data'  , df_tb.as_named_input('raw_data'), \n",
        "                                                '--output', pre_process_output_folder],   \n",
        "                                    outputs  = [pre_process_output_folder],\n",
        "                                    compute_target=compute, \n",
        "                                    runconfig=pipeline_runconfig, \n",
        "                                    allow_reuse=False, \n",
        "                                    source_directory='../src')\n",
        "\n",
        "model_prediction_step = PythonScriptStep(name = 'step 2: Model Prediction', \n",
        "                                    script_name='pima_modelPrediction_SDKv1.py',\n",
        "                                    arguments= ['--processed_data',pre_process_output_folder, \n",
        "                                                '--output', final_output_folder], \n",
        "                                    inputs   = [pre_process_output_folder], \n",
        "                                    outputs  = [final_output_folder], \n",
        "                                    compute_target=compute, \n",
        "                                    runconfig=pipeline_runconfig, \n",
        "                                    allow_reuse=False, \n",
        "                                    source_directory='../src') \n",
        "\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[pre_process_step,model_prediction_step]) \n",
        "pipeline.validate()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step step 1: Data Preparation is ready to be created [e71767ee]\nStep step 2: Model Prediction is ready to be created [e823f52c]\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1699879188695
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 5: Submit Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pipeline as an experiment\n",
        "pipeline_run = Experiment(ws, 'Pima_Inference_pipeline_SDKv1').submit(pipeline, continue_on_step_failure=True,)\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step step 1: Data Preparation [e71767ee][c2095bb7-8156-4ab8-a51e-8fda01853b51], (This step will run and generate new outputs)\nCreated step step 2: Model Prediction [e823f52c][de299d99-7a5f-4e36-944b-5998bfc6e4e7], (This step will run and generate new outputs)\nSubmitted PipelineRun 8f61236c-be3b-499c-90f9-45a5ddce1499\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/8f61236c-be3b-499c-90f9-45a5ddce1499?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure&tid=5ac231ff-07da-46e9-9b1d-c924625f23bd\nPipelineRunId: 8f61236c-be3b-499c-90f9-45a5ddce1499\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/8f61236c-be3b-499c-90f9-45a5ddce1499?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure&tid=5ac231ff-07da-46e9-9b1d-c924625f23bd\nPipelineRun Status: NotStarted\nPipelineRun Status: Running\n\n\nStepRunId: a97fbd61-886a-4866-b6bb-25264fde46e2\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/a97fbd61-886a-4866-b6bb-25264fde46e2?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure&tid=5ac231ff-07da-46e9-9b1d-c924625f23bd\nStepRun( step 1: Data Preparation ) Status: NotStarted\nStepRun( step 1: Data Preparation ) Status: Running\n\nStepRun(step 1: Data Preparation) Execution Summary\n====================================================\nStepRun( step 1: Data Preparation ) Status: Failed\n\nWarnings:\nAzureMLCompute job failed\nExecutionFailed: [REDACTED]\n\texit_codes: 1\n\tAppinsights Reachable: Some(true)\nExecution failed. User process '/azureml-envs/azureml_dde9de8808e90dc357d096bbcd2dcc0e/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error: Traceback (most recent call last):\n  File \"pima_inference_dataProcessing_SDKv1.py\", line 177, in <module>\n    processed_df = train_preprocessing(df)\n  File \"pima_inference_dataProcessing_SDKv1.py\", line 86, in train_preprocessing\n    if artifacts:\nNameError: name 'artifacts' is not defined\n\n\n"
        },
        {
          "output_type": "error",
          "ename": "ActivityFailedException",
          "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process '/azureml-envs/azureml_dde9de8808e90dc357d096bbcd2dcc0e/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error: Traceback (most recent call last):\\n  File \\\"pima_inference_dataProcessing_SDKv1.py\\\", line 177, in <module>\\n    processed_df = train_preprocessing(df)\\n  File \\\"pima_inference_dataProcessing_SDKv1.py\\\", line 86, in train_preprocessing\\n    if artifacts:\\nNameError: name 'artifacts' is not defined\\n\\n\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"componentName\": \"CommonRuntime\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Execution failed. User process '/azureml-envs/azureml_dde9de8808e90dc357d096bbcd2dcc0e/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error: Traceback (most recent call last):\\\\n  File \\\\\\\"pima_inference_dataProcessing_SDKv1.py\\\\\\\", line 177, in <module>\\\\n    processed_df = train_preprocessing(df)\\\\n  File \\\\\\\"pima_inference_dataProcessing_SDKv1.py\\\\\\\", line 86, in train_preprocessing\\\\n    if artifacts:\\\\nNameError: name 'artifacts' is not defined\\\\n\\\\n\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\",\\n    \\\"componentName\\\": \\\"CommonRuntime\\\"\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the pipeline as an experiment\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pipeline_run \u001b[38;5;241m=\u001b[39m Experiment(ws, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPima_Inference_pipeline_SDKv1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msubmit(pipeline, continue_on_step_failure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpipeline_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:295\u001b[0m, in \u001b[0;36mPipelineRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[43mstep_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_elapsed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# If there are package conflicts in the user's environment, the run rehydration\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# will not work and we will receive a Run object instead of StepRun.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# Run.wait_for_completion() does not have a parameter timeout_seconds, which\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# will generate a TypeError here.  As a workaround, call the method without\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# this parameter.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step_run, StepRun):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:738\u001b[0m, in \u001b[0;36mStepRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_output:\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_run_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m         error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe output streaming for the run interrupted.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    742\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut the run is still executing on the compute target. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    743\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails for canceling the run can be found here: \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    744\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:831\u001b[0m, in \u001b[0;36mStepRun._stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m raise_on_error:\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ActivityFailedException(error_details\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_details)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process '/azureml-envs/azureml_dde9de8808e90dc357d096bbcd2dcc0e/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error: Traceback (most recent call last):\\n  File \\\"pima_inference_dataProcessing_SDKv1.py\\\", line 177, in <module>\\n    processed_df = train_preprocessing(df)\\n  File \\\"pima_inference_dataProcessing_SDKv1.py\\\", line 86, in train_preprocessing\\n    if artifacts:\\nNameError: name 'artifacts' is not defined\\n\\n\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"componentName\": \"CommonRuntime\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Execution failed. User process '/azureml-envs/azureml_dde9de8808e90dc357d096bbcd2dcc0e/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error: Traceback (most recent call last):\\\\n  File \\\\\\\"pima_inference_dataProcessing_SDKv1.py\\\\\\\", line 177, in <module>\\\\n    processed_df = train_preprocessing(df)\\\\n  File \\\\\\\"pima_inference_dataProcessing_SDKv1.py\\\\\\\", line 86, in train_preprocessing\\\\n    if artifacts:\\\\nNameError: name 'artifacts' is not defined\\\\n\\\\n\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\",\\n    \\\"componentName\\\": \\\"CommonRuntime\\\"\\n}\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1699879244667
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 6: Creating an endpoint and publishing the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineEndpoint\n",
        "\n",
        "# The pipeline argument can be either a Pipeline or a PublishedPipeline\n",
        "pipeline_endpoint = PipelineEndpoint.publish(workspace=ws,\n",
        "                                             name=\"pima_sdkV1_prediction_PipelineEdnpoint\",\n",
        "                                             pipeline=pipeline,\n",
        "                                             description=\"pipeline to predict data\")\n",
        "\n",
        "                                               "
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1696481067225
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## publish it to same endpoint when the pipeline is enhanced or modified\n",
        "\n",
        "# published_pipeline = pipeline_run.publish_pipeline(name ='predictionPipline_v2',\n",
        "#                                                    description='pipeline to predict data',\n",
        "#                                                    version='2')\n",
        "\n",
        "# pipeline_endpoint = PipelineEndpoint.get(workspace=ws, name=\"prediction_PipelineEdnpoint\")\n",
        "# pipeline_endpoint.add_default(published_pipeline)\n",
        "# print(pipeline_endpoint.endpoint)\n",
        "# pipeline_endpoint"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696481133270
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 7: Invoking Endpoint\n",
        "\n",
        "To run the pipeline from the REST endpoint, you first need an OAuth2 Bearer-type authentication header. This example uses interactive authentication for illustration purposes, but for most production scenarios requiring automated or headless authentication, use service principle authentication as described in this notebook.\n",
        "\n",
        "Service principle authentication involves creating an App Registration in Azure Active Directory, generating a client secret, and then granting your service principal role access to your machine learning workspace. You then use the ServicePrincipalAuthentication class to manage your auth flow.\n",
        "\n",
        "Both InteractiveLoginAuthentication and ServicePrincipalAuthentication inherit from AbstractAuthentication, and in both cases you use the get_authentication_header() function in the same way to fetch the header"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import requests\n",
        "\n",
        "# Authentication\n",
        "interactive_authentication = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_authentication.get_authentication_header()\n",
        "print('authentication header ready')\n",
        "\n",
        "response = requests.post(pipeline_endpoint.endpoint,\n",
        "                         headers=auth_header,\n",
        "                         json={\"ExperimentName\": \"Experiments_Training\"})\n",
        "run_id = response.json()[\"Id\"]\n",
        "# print('pipeline invoked:',)\n",
        "print(auth_header)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696520844643
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments[\"Experiments_Training\"], run_id)\n",
        "published_pipeline_run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1696481266660
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}