{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Workflow\n",
        "1. Initialize Workspace & creat workspace handle\n",
        "2. Initialize\n",
        "    - compute Cluster \n",
        "    - Environment\n",
        "3. Create a .py scripts Data Processing & Training Model\n",
        "4. Create Components\n",
        "5. Build Pipeline using Components\n",
        "6. Get Data Path\n",
        "7. Initiate Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Initialize Workspace and Create Workspace handle"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Initialize  workspace\n",
        "ws = Workspace.from_config()  \n",
        "\n",
        "# Get a handle to the workspace\n",
        "credential = DefaultAzureCredential()  # authenticate\n",
        "ml_client = MLClient( credential=credential,\n",
        "                      subscription_id=ws.subscription_id,\n",
        "                      resource_group_name=ws.resource_group,\n",
        "                      workspace_name=ws.name,\n",
        "                    )\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1700053561715
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Initialize Compute Cluster & Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "compute = \"ML-Pipeline-Cluster\"\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ml_client.compute.get(compute)\n",
        "    print(f\"You already have a cluster named {compute}, we'll reuse it as is.\")\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=compute,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "        idle_time_before_scale_down=300,\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    \n",
        "    print(f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\")\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named ML-Pipeline-Cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700053562354
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name  = \"ENV-SDKv2\"\n",
        "# dependencies_dir = '../dependencies'\n",
        "# env = Environment( name=custom_env_name,\n",
        "#                    description=\"Evironment for python SDKv2 Execution\",\n",
        "#                    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "#                    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "#                  )\n",
        "# env = ml_client.environments.create_or_update(env)\n",
        "\n",
        "# GET ENVIRONMENT\n",
        "# use 'label' parameter to get latest environment for example label='latest'\n",
        "# use 'version' parameter to get specific version environment, for example version=2\n",
        "env = ml_client.environments.get(name=custom_env_name, label='latest') \n",
        "\n",
        "print(f\"Environment with name {env.name} is registered to workspace, the environment version is {env.version}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name ENV-SDKv2 is registered to workspace, the environment version is 9\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700053563535
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Create Components to Build Pipeline\n",
        "\n",
        "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself.\n",
        "\n",
        "Azure Machine Learning pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n",
        "\n",
        "- Write the yaml specification of the component, or create it programmatically using `ComponentMethod`.\n",
        "- Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\n",
        "- Load that component from the pipeline code.\n",
        "- Implement the pipeline using the component's inputs, outputs and parameters.\n",
        "- Submit the pipeline.\n",
        "\n",
        "There are two ways to create a component, programmatic and yaml definition. The next two sections walk you through creating a component using programmatic definition\n",
        "\n",
        "> [!NOTE]\n",
        "> In this tutorial for simplicity we are using the same compute for all components. However, you can set different computes for each component, for example by adding a line like `train_step.compute = \"cpu-cluster\"`. To view an example of building a pipeline with different computes for each component, see the [Basic pipeline job section in the cifar-10 pipeline tutorial](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/2b_train_cifar_10_with_pytorch/train_cifar_10_with_pytorch.ipynb)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "scripts_dir = \"../src\"\n",
        "data_prep_component = command( name=\"inference data prep pima diabetes detection\",\n",
        "                               display_name =\"Data preparation for inference\",\n",
        "                               description  =\"reads input data & preprocesses it\",\n",
        "                               inputs= { \"data\": Input(type=\"uri_folder\")},\n",
        "\n",
        "                               outputs=dict( processed_data=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
        "                               code=scripts_dir, # The source folder of the component\n",
        "                               command=\"\"\"python pima_inference_dataProcessing_SDKv2.py \\\n",
        "                                        --data ${{inputs.data}} \\\n",
        "                                        --output ${{outputs.processed_data}} \\\n",
        "                                        \"\"\",\n",
        "                               environment=f\"{env.name}:{env.version}\",\n",
        "                            )\n",
        "\n",
        "# , \"output\": Input(type=\"uri_folder\")\n",
        "\n",
        "prediction_component = command( name=\"pima diabetes model inference\",\n",
        "                            display_name =\"Model inference\",\n",
        "                            inputs= { \"processed_data\": Input(type=\"uri_folder\")\n",
        "                                    },\n",
        "                            outputs=dict(output=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
        "\n",
        "                            code=scripts_dir,\n",
        "                            command=\"\"\"python pima_modelPrediction_SDKv2.py \\\n",
        "                                    --processed_data ${{inputs.processed_data}} \\\n",
        "                                    --output ${{outputs.output}} \\\n",
        "                                    \"\"\",\n",
        "                            environment=f\"{env.name}:{env.version}\",\n",
        "                            )"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700054474415
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 4: Build Pipeline using Components\n",
        "\n",
        "To code the pipeline, you use a specific `@dsl.pipeline` decorator that identifies the Azure Machine Learning pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage. Like a Python function, pipelines can have inputs. You can then create multiple instances of a single pipeline with different inputs.\n",
        "\n",
        "Here, we used *input data*, *split ratio* and *registered model name* as input variables. We then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl\n",
        "\n",
        "@dsl.pipeline(compute=compute, description=\"Building pima inference Pipeline using SDKv2\")\n",
        "def pima_inference_pipeline(input_data):\n",
        "                             # using data_prep_function like a python call with its own inputs\n",
        "                             data_prep_job = data_prep_component(data=input_data)\n",
        "\n",
        "                             # using train_func like a python call with its own inputs\n",
        "                             prediction_job = prediction_component( processed_data  = data_prep_job.outputs.processed_data,     # note: using outputs from previous step\n",
        "                                                          \n",
        "                                                        )\n",
        "\n",
        "                             # a pipeline returns a dictionary of outputs\n",
        "                             # return  { \"processed_data\": data_prep_job.outputs.processed_data }"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700054475130
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 6: Get Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FETCH DATA\n",
        "dataset_name = \"test_pima_data_typeFile_SDKv2\"  \n",
        "pima_data  = ml_client.data.get(name = dataset_name, label = \"latest\")"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700054477260
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 7: Initiate Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the model to be registered as \n",
        "#registered_model_name = \"pima_pipeline_model_SDKv2\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = pima_inference_pipeline(input_data=Input(type=\"uri_file\", path= pima_data.path))\n",
        "                                                                       "
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700054478264
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 8: Submit Job"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(pipeline,experiment_name=\"pima_pipeline_inference_sdk_v2\",)\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "\n\u001b[37m\n\u001b[30m\n1) At least one required parameter is missing\u001b[39m\u001b[39m\n\nDetails: \n\n\u001b[31m(x) Input path can't be empty for jobs.\u001b[39m\n\nResolutions: \n1) Ensure all parameters required by the Job schema are specified.\nIf using the CLI, you can also check the full log in debug mode for more details by adding --debug to the end of your command\n\nAdditional Resources: The easiest way to author a yaml specification file is using IntelliSense and auto-completion Azure ML VS code extension provides: \u001b[36mhttps://code.visualstudio.com/docs/datascience/azure-machine-learning.\u001b[39m To set up VS Code, visit \u001b[36mhttps://docs.microsoft.com/azure/machine-learning/how-to-setup-vs-code\u001b[39m\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:541\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[0;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# Create all dependent resources\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_or_upload_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m git_props \u001b[38;5;241m=\u001b[39m get_git_properties()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:890\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_upload_dependencies\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This method converts name or name:version to ARM id. Or it\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;124;03mregisters/uploads nested dependencies.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m:rtype: Job\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_or_azureml_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orchestrators\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_asset_arm_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, PipelineJob):\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# Resolve top-level inputs\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:1115\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_azureml_id\u001b[0;34m(self, job, resolver)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job, PipelineJob):\n\u001b[0;32m-> 1115\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_arm_id_for_pipeline_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:1230\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_for_pipeline_job\u001b[0;34m(self, pipeline_job, resolver)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_component_operations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_dependencies_for_pipeline_component_jobs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolver\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComponentException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_component_operations.py:741\u001b[0m, in \u001b[0;36mComponentOperations._resolve_dependencies_for_pipeline_component_jobs\u001b[0;34m(self, component, resolver, resolve_inputs)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_inputs:\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_inputs_for_pipeline_component_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# This is a preparation for concurrent resolution. Nodes will be resolved later layer by layer\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# from bottom to top, as hash calculation of a parent node will be impacted by resolution\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# of its child nodes.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_component_operations.py:568\u001b[0m, in \u001b[0;36mComponentOperations._resolve_inputs_for_pipeline_component_jobs\u001b[0;34m(self, jobs, base_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m     node: BaseNode \u001b[38;5;241m=\u001b[39m job_instance\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_job_operations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_job_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(job_instance, AutoMLJob):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:972\u001b[0m, in \u001b[0;36mJobOperations._resolve_job_inputs\u001b[0;34m(self, entries, base_path)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n\u001b[0;32m--> 972\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_job_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:995\u001b[0m, in \u001b[0;36mJobOperations._resolve_job_input\u001b[0;34m(self, entry, base_path)\u001b[0m\n\u001b[1;32m    994\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput path can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be empty for jobs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationException(\n\u001b[1;32m    996\u001b[0m         message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m    997\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    998\u001b[0m         no_personal_data_message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m    999\u001b[0m         error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m   1000\u001b[0m         error_type\u001b[38;5;241m=\u001b[39mValidationErrorType\u001b[38;5;241m.\u001b[39mMISSING_FIELD,\n\u001b[1;32m   1001\u001b[0m     )\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(entry, Input)\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_ARM_id_for_resource(entry\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_url(entry\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_data_binding_expression(entry\u001b[38;5;241m.\u001b[39mpath)  \u001b[38;5;66;03m# literal value but set mode in pipeline yaml\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m ):  \u001b[38;5;66;03m# Literal value, ARM id or remote url. Pass through\u001b[39;00m\n",
            "\u001b[0;31mValidationException\u001b[0m: Input path can't be empty for jobs.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# submit the pipeline job\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pipeline_job \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpima_pipeline_inference_sdk_v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mstream(pipeline_job\u001b[38;5;241m.\u001b[39mname)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:337\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameter_dimensions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(custom_dimensions \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, dimensions) \u001b[38;5;28;01mas\u001b[39;00m activityLogger:\n\u001b[0;32m--> 337\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter_dimensions:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;66;03m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[1;32m    340\u001b[0m         activityLogger\u001b[38;5;241m.\u001b[39mactivity_info\u001b[38;5;241m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:607\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[0;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarshmallow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m SchemaValidationError\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ex, (ValidationException, SchemaValidationError)):\n\u001b[0;32m--> 607\u001b[0m     \u001b[43mlog_and_raise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_exception_helper.py:295\u001b[0m, in \u001b[0;36mlog_and_raise_error\u001b[0;34m(error, debug, yaml_operation)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(formatted_error)\n",
            "\u001b[0;31mException\u001b[0m: \n\u001b[37m\n\u001b[30m\n1) At least one required parameter is missing\u001b[39m\u001b[39m\n\nDetails: \n\n\u001b[31m(x) Input path can't be empty for jobs.\u001b[39m\n\nResolutions: \n1) Ensure all parameters required by the Job schema are specified.\nIf using the CLI, you can also check the full log in debug mode for more details by adding --debug to the end of your command\n\nAdditional Resources: The easiest way to author a yaml specification file is using IntelliSense and auto-completion Azure ML VS code extension provides: \u001b[36mhttps://code.visualstudio.com/docs/datascience/azure-machine-learning.\u001b[39m To set up VS Code, visit \u001b[36mhttps://docs.microsoft.com/azure/machine-learning/how-to-setup-vs-code\u001b[39m\n"
          ]
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700054479984
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from azureml.core import Run, Model\n",
        "# #run= Run.get_context()\n",
        "# #ws = run.experiment.workspace\n",
        "            \n",
        "# model_obj  = Model(ws, name= 'pima_pipeline_model_SDKv2_03') # by default takes the latest version\n",
        "# artifacts_path = model_obj.download(exist_ok = True)\n",
        "\n",
        "# import joblib\n",
        "# joblib.load(os.path.join(artifacts_path, 'unique_values_train.pkl'))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "{'BM_DESC': ['Obese', 'Over', 'Healthy', 'Under']\n Categories (4, object): ['Obese', 'Over', 'Healthy', 'Under'],\n 'INSULIN_DESC': ['Normal', 'Abnormal']\n Categories (2, object): ['Normal', 'Abnormal']}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699970421774
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}