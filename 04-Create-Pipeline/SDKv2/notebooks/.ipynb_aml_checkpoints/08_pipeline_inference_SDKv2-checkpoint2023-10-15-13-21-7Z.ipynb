{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Workflow\n",
        "1. Initialize Workspace & creat workspace handle\n",
        "2. Initialize\n",
        "    - compute Cluster \n",
        "    - Environment\n",
        "3. Create a .py scripts Data Processing & Training Model\n",
        "4. Create Components\n",
        "5. Build Pipeline using Components\n",
        "6. Get Data Path\n",
        "7. Initiate Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Initialize Workspace and Create Workspace handle"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Initialize  workspace\n",
        "ws = Workspace.from_config()  \n",
        "\n",
        "# Get a handle to the workspace\n",
        "credential = DefaultAzureCredential()  # authenticate\n",
        "ml_client = MLClient( credential=credential,\n",
        "                      subscription_id=ws.subscription_id,\n",
        "                      resource_group_name=ws.resource_group,\n",
        "                      workspace_name=ws.name,\n",
        "                    )\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1700053561715
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Initialize Compute Cluster & Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "compute = \"ML-Pipeline-Cluster\"\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ml_client.compute.get(compute)\n",
        "    print(f\"You already have a cluster named {compute}, we'll reuse it as is.\")\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=compute,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "        idle_time_before_scale_down=300,\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    \n",
        "    print(f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\")\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named ML-Pipeline-Cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700053562354
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name  = \"ENV-SDKv2\"\n",
        "# dependencies_dir = '../dependencies'\n",
        "# env = Environment( name=custom_env_name,\n",
        "#                    description=\"Evironment for python SDKv2 Execution\",\n",
        "#                    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "#                    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "#                  )\n",
        "# env = ml_client.environments.create_or_update(env)\n",
        "\n",
        "# GET ENVIRONMENT\n",
        "# use 'label' parameter to get latest environment for example label='latest'\n",
        "# use 'version' parameter to get specific version environment, for example version=2\n",
        "env = ml_client.environments.get(name=custom_env_name, label='latest') \n",
        "\n",
        "print(f\"Environment with name {env.name} is registered to workspace, the environment version is {env.version}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name ENV-SDKv2 is registered to workspace, the environment version is 9\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700053563535
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Create Components to Build Pipeline\n",
        "\n",
        "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself.\n",
        "\n",
        "Azure Machine Learning pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n",
        "\n",
        "- Write the yaml specification of the component, or create it programmatically using `ComponentMethod`.\n",
        "- Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\n",
        "- Load that component from the pipeline code.\n",
        "- Implement the pipeline using the component's inputs, outputs and parameters.\n",
        "- Submit the pipeline.\n",
        "\n",
        "There are two ways to create a component, programmatic and yaml definition. The next two sections walk you through creating a component using programmatic definition\n",
        "\n",
        "> [!NOTE]\n",
        "> In this tutorial for simplicity we are using the same compute for all components. However, you can set different computes for each component, for example by adding a line like `train_step.compute = \"cpu-cluster\"`. To view an example of building a pipeline with different computes for each component, see the [Basic pipeline job section in the cifar-10 pipeline tutorial](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/2b_train_cifar_10_with_pytorch/train_cifar_10_with_pytorch.ipynb)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "scripts_dir = \"../src\"\n",
        "data_prep_component = command( name=\"inference data prep pima diabetes detection\",\n",
        "                               display_name =\"Data preparation for inference\",\n",
        "                               description  =\"reads input data & preprocesses it\",\n",
        "                               inputs= { \"data\": Input(type=\"uri_folder\")},\n",
        "\n",
        "                               outputs=dict( processed_data=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
        "                               code=scripts_dir, # The source folder of the component\n",
        "                               command=\"\"\"python pima_inference_dataProcessing_SDKv2.py \\\n",
        "                                        --data ${{inputs.data}} \\\n",
        "                                        --output ${{outputs.processed_data}} \\\n",
        "                                        \"\"\",\n",
        "                               environment=f\"{env.name}:{env.version}\",\n",
        "                            )\n",
        "\n",
        "prediction_component = command( name=\"pima diabetes model inference\",\n",
        "                            display_name =\"Model inference\",\n",
        "                            inputs= { \"processed_data\": Input(type=\"uri_folder\"),\n",
        "                            \"output\": Input(type=\"uri_folder\")\n",
        "                                    },\n",
        "                            outputs=dict(output=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
        "\n",
        "                            code=scripts_dir,\n",
        "                            command=\"\"\"python pima_modelPrediction_SDKv2.py \\\n",
        "                                    --input_data ${{inputs.processed_data}} \\\n",
        "                                    --output ${{outputs.output}} \\\n",
        "                                    \"\"\",\n",
        "                            environment=f\"{env.name}:{env.version}\",\n",
        "                            )"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700053929592
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 4: Build Pipeline using Components\n",
        "\n",
        "To code the pipeline, you use a specific `@dsl.pipeline` decorator that identifies the Azure Machine Learning pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage. Like a Python function, pipelines can have inputs. You can then create multiple instances of a single pipeline with different inputs.\n",
        "\n",
        "Here, we used *input data*, *split ratio* and *registered model name* as input variables. We then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl\n",
        "\n",
        "@dsl.pipeline(compute=compute, description=\"Building pima inference Pipeline using SDKv2\")\n",
        "def pima_inference_pipeline(input_data):\n",
        "                             # using data_prep_function like a python call with its own inputs\n",
        "                             data_prep_job = data_prep_component(data=input_data)\n",
        "\n",
        "                             # using train_func like a python call with its own inputs\n",
        "                             prediction_job = prediction_component( processed_data  = data_prep_job.outputs.processed_data,     # note: using outputs from previous step\n",
        "                                                          \n",
        "                                                        )\n",
        "\n",
        "                             # a pipeline returns a dictionary of outputs\n",
        "                             # return  { \"processed_data\": data_prep_job.outputs.processed_data }"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700053930799
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 6: Get Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FETCH DATA\n",
        "dataset_name = \"test_pima_data_typeFile_SDKv2\"  \n",
        "pima_data  = ml_client.data.get(name = dataset_name, label = \"latest\")"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700053932599
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 7: Initiate Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the model to be registered as \n",
        "#registered_model_name = \"pima_pipeline_model_SDKv2\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = pima_inference_pipeline(input_data=Input(type=\"uri_file\", path= pima_data.path))\n",
        "                                                                       "
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700053933116
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 8: Submit Job"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(pipeline,experiment_name=\"pima_pipeline_inference_sdk_v2\",)\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading src (0.02 MBs):   0%|          | 0/20442 [00:00<?, ?it/s]\r\u001b[32mUploading src (0.02 MBs):  28%|██▊       | 5790/20442 [00:00<00:00, 46977.05it/s]\r\u001b[32mUploading src (0.02 MBs): 100%|██████████| 20442/20442 [00:00<00:00, 139821.31it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: bubbly_cumin_84j48k0jsl\nWeb View: https://ml.azure.com/runs/bubbly_cumin_84j48k0jsl?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-11-15 13:12:48Z] Submitting 1 runs, first five are: 61b63a7e:7b7f1a6f-e324-4dc2-badb-8fc6468c15d0\n[2023-11-15 13:13:36Z] Completing processing run id 7b7f1a6f-e324-4dc2-badb-8fc6468c15d0.\n[2023-11-15 13:13:37Z] Submitting 1 runs, first five are: af8c3aed:43f848ed-1e67-4f12-b90c-1e47a498fd98\n[2023-11-15 13:14:16Z] Execution of experiment failed, update experiment status and cancel running nodes.\n\nExecution Summary\n=================\nRunId: bubbly_cumin_84j48k0jsl\nWeb View: https://ml.azure.com/runs/bubbly_cumin_84j48k0jsl?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure\n"
        },
        {
          "output_type": "error",
          "ename": "JobException",
          "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /prediction_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"centralindia\",\n    \"location\": \"centralindia\",\n    \"time\": \"2023-11-15T13:14:16.142124Z\",\n    \"component_name\": \"\"\n} ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# submit the pipeline job\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pipeline_job \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate_or_update(pipeline,experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpima_pipeline_inference_sdk_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:661\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 661\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_ops_helper.py:312\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    310\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[1;32m    313\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)),\n\u001b[1;32m    314\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    315\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException raised on failed job.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    316\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    319\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
            "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /prediction_job. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"centralindia\",\n    \"location\": \"centralindia\",\n    \"time\": \"2023-11-15T13:14:16.142124Z\",\n    \"component_name\": \"\"\n} "
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700054049027
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from azureml.core import Run, Model\n",
        "# #run= Run.get_context()\n",
        "# #ws = run.experiment.workspace\n",
        "            \n",
        "# model_obj  = Model(ws, name= 'pima_pipeline_model_SDKv2_03') # by default takes the latest version\n",
        "# artifacts_path = model_obj.download(exist_ok = True)\n",
        "\n",
        "# import joblib\n",
        "# joblib.load(os.path.join(artifacts_path, 'unique_values_train.pkl'))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "{'BM_DESC': ['Obese', 'Over', 'Healthy', 'Under']\n Categories (4, object): ['Obese', 'Over', 'Healthy', 'Under'],\n 'INSULIN_DESC': ['Normal', 'Abnormal']\n Categories (2, object): ['Normal', 'Abnormal']}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699970421774
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}