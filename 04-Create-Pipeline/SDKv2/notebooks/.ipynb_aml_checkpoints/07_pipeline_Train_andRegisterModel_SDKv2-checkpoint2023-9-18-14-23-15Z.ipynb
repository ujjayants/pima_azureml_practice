{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Workflow\n",
        "1. Initialize Workspace & creat workspace handle\n",
        "2. Initialize\n",
        "    - compute Cluster \n",
        "    - Environment\n",
        "3. Create a .py scripts Data Processing & Training Model\n",
        "4. Create Components\n",
        "5. Build Pipeline using Components\n",
        "6. Get Data Path\n",
        "7. Initiate Pipeline"
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Initialize Workspace and Create Workspace handle"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Initialize  workspace\n",
        "ws = Workspace.from_config()  \n",
        "\n",
        "# Get a handle to the workspace\n",
        "credential = DefaultAzureCredential()  # authenticate\n",
        "ml_client = MLClient( credential=credential,\n",
        "                      subscription_id=ws.subscription_id,\n",
        "                      resource_group_name=ws.resource_group,\n",
        "                      workspace_name=ws.name,\n",
        "                    )\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1697606438793
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Initialize Compute Cluster & Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "compute = \"ML-Pipeline-Cluster\"\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ml_client.compute.get(compute)\n",
        "    print(f\"You already have a cluster named {compute}, we'll reuse it as is.\")\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=compute,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "        idle_time_before_scale_down=300,\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    \n",
        "    print(f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\")\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named ML-Pipeline-Cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1697606442455
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name  = \"ENV-SDKv2\"\n",
        "# dependencies_dir = '../dependencies'\n",
        "# env = Environment( name=custom_env_name,\n",
        "#                    description=\"Evironment for python SDKv2 Execution\",\n",
        "#                    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "#                    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "#                  )\n",
        "# env = ml_client.environments.create_or_update(env)\n",
        "\n",
        "# GET ENVIRONMENT\n",
        "# use 'label' parameter to get latest environment for example label='latest'\n",
        "# use 'version' parameter to get specific version environment, for example version=2\n",
        "env = ml_client.environments.get(name=custom_env_name, label='latest') \n",
        "\n",
        "print(f\"Environment with name {env.name} is registered to workspace, the environment version is {env.version}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name ENV-SDKv2 is registered to workspace, the environment version is 1\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1697606447663
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Create Components to Build Pipeline\n",
        "\n",
        "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself.\n",
        "\n",
        "Azure Machine Learning pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n",
        "\n",
        "- Write the yaml specification of the component, or create it programmatically using `ComponentMethod`.\n",
        "- Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\n",
        "- Load that component from the pipeline code.\n",
        "- Implement the pipeline using the component's inputs, outputs and parameters.\n",
        "- Submit the pipeline.\n",
        "\n",
        "There are two ways to create a component, programmatic and yaml definition. The next two sections walk you through creating a component using programmatic definition\n",
        "\n",
        "> [!NOTE]\n",
        "> In this tutorial for simplicity we are using the same compute for all components. However, you can set different computes for each component, for example by adding a line like `train_step.compute = \"cpu-cluster\"`. To view an example of building a pipeline with different computes for each component, see the [Basic pipeline job section in the cifar-10 pipeline tutorial](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/2b_train_cifar_10_with_pytorch/train_cifar_10_with_pytorch.ipynb)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "scripts_dir = \"../src\"\n",
        "data_prep_component = command( name=\"Data prep CreditFraud Detection\",\n",
        "                               display_name =\"Data preparation for training\",\n",
        "                               description  =\"reads input data & preprocesses it\",\n",
        "                               inputs= { \"data\": Input(type=\"uri_folder\") },\n",
        "                               outputs=dict( processed_data=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
        "                               code=scripts_dir, # The source folder of the component\n",
        "                               command=\"\"\"python dataProcessing_SDKv2.py \\\n",
        "                                        --data ${{inputs.data}} \\\n",
        "                                        --processed_data ${{outputs.processed_data}} \\\n",
        "                                        \"\"\",\n",
        "                               environment=f\"{env.name}:{env.version}\",\n",
        "                            )\n",
        "\n",
        "train_component = command( name=\"Training  Model\",\n",
        "                            display_name =\"Training Model\",\n",
        "                            inputs= { \"processed_data\": Input(type=\"uri_folder\"),\n",
        "                                      \"test_train_ratio\": Input(type='number'),\n",
        "                                      \"registered_model_name\":Input(type='string'),\n",
        "                                    },\n",
        "                            outputs=dict(model=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
        "                            code=scripts_dir,\n",
        "                            command=\"\"\"python model_Train_andRegister_SDKv2.py \\\n",
        "                                    --input_data ${{inputs.processed_data}} \\\n",
        "                                    --registered_model_name ${{inputs.registered_model_name}} \\\n",
        "                                    --model ${{outputs.model}} \\\n",
        "                                    \"\"\",\n",
        "                            environment=f\"{env.name}:{env.version}\",\n",
        "                            )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Changing node name Data prep CreditFraud Detection to lower case: data prep creditfraud detection since upper case is not allowed node name.\nChanging node name Training  Model to lower case: training  model since upper case is not allowed node name.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1697606469651
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 4: Build Pipeline using Components\n",
        "\n",
        "To code the pipeline, you use a specific `@dsl.pipeline` decorator that identifies the Azure Machine Learning pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage. Like a Python function, pipelines can have inputs. You can then create multiple instances of a single pipeline with different inputs.\n",
        "\n",
        "Here, we used *input data*, *split ratio* and *registered model name* as input variables. We then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
        "from azure.ai.ml import dsl\n",
        "\n",
        "@dsl.pipeline(compute=compute, description=\"Building Training Pipeline using SDKv2\")\n",
        "def credit_fraud_detection_pipeline(input_data, test_train_ratio, registered_model_name,):\n",
        "                             # using data_prep_function like a python call with its own inputs\n",
        "                             data_prep_job = data_prep_component(data=input_data)\n",
        "\n",
        "                             # using train_func like a python call with its own inputs\n",
        "                             train_job = train_component( processed_data  = data_prep_job.outputs.processed_data,     # note: using outputs from previous step\n",
        "                                                          test_train_ratio=test_train_ratio,\n",
        "                                                          registered_model_name=registered_model_name,\n",
        "                                                        )\n",
        "\n",
        "                             # a pipeline returns a dictionary of outputs\n",
        "                             # return  { \"processed_data\": data_prep_job.outputs.processed_data }"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1697606502961
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 6: Get Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# FETCH DATA\n",
        "dataset_name = \"credit_data_typeFile_SDK_v2\"  \n",
        "credit_data  = ml_client.data.get(name = dataset_name, label = \"latest\")"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1697606574697
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 7: Initiate Pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the model to be registered as \n",
        "registered_model_name = \"credit_defaults_model_SDKv2\"\n",
        "\n",
        "# Let's instantiate the pipeline with the parameters of our choice\n",
        "pipeline = credit_fraud_detection_pipeline(input_data=Input(type=\"uri_file\", path=credit_data.path),\n",
        "                                    test_train_ratio=0.25,\n",
        "                                    registered_model_name=registered_model_name,\n",
        "                                    )"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1697606582396
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Step 8: Submit Job"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(pipeline,experiment_name=\"Experiments_Training\",)\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nUploading src (0.0 MBs): 100%|██████████| 4237/4237 [00:00<00:00, 43013.66it/s]\n\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: bright_camel_3gjm2zny9w\nWeb View: https://ml.azure.com/runs/bright_camel_3gjm2zny9w?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-10-18 05:23:31Z] Submitting 1 runs, first five are: ce85a02d:3f15b4ca-08bd-4242-9836-60d0f7454471\n[2023-10-18 05:28:23Z] Completing processing run id 3f15b4ca-08bd-4242-9836-60d0f7454471.\n[2023-10-18 05:28:24Z] Submitting 1 runs, first five are: f3fba24f:3559da2a-aabd-4d0c-92e6-566c840e91ba\n[2023-10-18 05:29:49Z] Completing processing run id 3559da2a-aabd-4d0c-92e6-566c840e91ba.\n\nExecution Summary\n=================\nRunId: bright_camel_3gjm2zny9w\nWeb View: https://ml.azure.com/runs/bright_camel_3gjm2zny9w?wsid=/subscriptions/ba5d6a04-af22-45ea-bc5a-946ef1c32949/resourcegroups/us_azure_practice/workspaces/us_azure\n\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1697606998683
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}