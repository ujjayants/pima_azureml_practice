{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Workflow\n",
        "1. Initialize Workspace & creat workspace handle\n",
        "2. Initialize\n",
        "    - compute Cluster \n",
        "    - Environment\n",
        "3. Create a .py scripts Data Processing & Training Model\n",
        "4. Create Components\n",
        "5. Build Pipeline using Components\n",
        "6. Get Data Path\n",
        "7. Initiate Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Initialize Workspace and Create Workspace handle"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Initialize  workspace\n",
        "ws = Workspace.from_config()  \n",
        "\n",
        "# Get a handle to the workspace\n",
        "credential = DefaultAzureCredential()  # authenticate\n",
        "ml_client = MLClient( credential=credential,\n",
        "                      subscription_id=ws.subscription_id,\n",
        "                      resource_group_name=ws.resource_group,\n",
        "                      workspace_name=ws.name,\n",
        "                    )\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Initialize Compute Cluster & Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "compute = \"ML-Pipeline-Cluster\"\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ml_client.compute.get(compute)\n",
        "    print(f\"You already have a cluster named {compute}, we'll reuse it as is.\")\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=compute,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "        idle_time_before_scale_down=300,\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    \n",
        "    print(f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\")\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name  = \"ENV-SDKv2\"\n",
        "# dependencies_dir = '../dependencies'\n",
        "# env = Environment( name=custom_env_name,\n",
        "#                    description=\"Evironment for python SDKv2 Execution\",\n",
        "#                    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "#                    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "#                  )\n",
        "# env = ml_client.environments.create_or_update(env)\n",
        "\n",
        "# GET ENVIRONMENT\n",
        "# use 'label' parameter to get latest environment for example label='latest'\n",
        "# use 'version' parameter to get specific version environment, for example version=2\n",
        "env = ml_client.environments.get(name=custom_env_name, label='latest') \n",
        "\n",
        "print(f\"Environment with name {env.name} is registered to workspace, the environment version is {env.version}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Create Components to Build Pipeline\n",
        "\n",
        "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself.\n",
        "\n",
        "Azure Machine Learning pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\n",
        "\n",
        "- Write the yaml specification of the component, or create it programmatically using `ComponentMethod`.\n",
        "- Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\n",
        "- Load that component from the pipeline code.\n",
        "- Implement the pipeline using the component's inputs, outputs and parameters.\n",
        "- Submit the pipeline.\n",
        "\n",
        "There are two ways to create a component, programmatic and yaml definition. The next two sections walk you through creating a component using programmatic definition\n",
        "\n",
        "> [!NOTE]\n",
        "> In this tutorial for simplicity we are using the same compute for all components. However, you can set different computes for each component, for example by adding a line like `train_step.compute = \"cpu-cluster\"`. To view an example of building a pipeline with different computes for each component, see the [Basic pipeline job section in the cifar-10 pipeline tutorial](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/2b_train_cifar_10_with_pytorch/train_cifar_10_with_pytorch.ipynb)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "scripts_dir = \"../src\"\n",
        "data_prep_component = command( name=\"data prep pima diabetes detection\",\n",
        "                               display_name =\"Data preparation for inference\",\n",
        "                               description  =\"reads input data & preprocesses it\",\n",
        "                               inputs= { \"data\": Input(type=\"uri_folder\")},\n",
        "\n",
        "                               outputs=dict( processed_data=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
        "                               code=scripts_dir, # The source folder of the component\n",
        "                               command=\"\"\"python pima_inference_dataProcessing_SDKv2.py \\\n",
        "                                        --data ${{inputs.data}} \\\n",
        "                                        --processed_data ${{outputs.processed_data}} \\\n",
        "                                        \"\"\",\n",
        "                               environment=f\"{env.name}:{env.version}\",\n",
        "                            )\n",
        "\n",
        "train_component = command( name=\"pima diabetes model inference\",\n",
        "                            display_name =\"Model inference\",\n",
        "                            inputs= { \"processed_data\": Input(type=\"uri_folder\"),\n",
        "                                      \"registered_model_name\":Input(type='string'),\n",
        "                                    },\n",
        "                            outputs=dict(model=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
        "                            code=scripts_dir,\n",
        "                            command=\"\"\"python pima_model_inference_SDKv2.py \\\n",
        "                                    --input_data ${{inputs.processed_data}} \\\n",
        "                                    --registered_model_name ${{inputs.registered_model_name}} \\\n",
        "                                    --model ${{outputs.model}} \\\n",
        "                                    \"\"\",\n",
        "                            environment=f\"{env.name}:{env.version}\",\n",
        "                            )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}